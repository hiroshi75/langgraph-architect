# çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œè¨¼

LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è©•ä¾¡ã«ãŠã‘ã‚‹çµ±è¨ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æœ‰æ„æ€§æ¤œå®šã€‚

## ğŸ“ˆ è¤‡æ•°å›å®Ÿè¡Œã®é‡è¦æ€§

### ãªãœè¤‡æ•°å›å®Ÿè¡ŒãŒå¿…è¦ã‹

1. **ãƒ©ãƒ³ãƒ€ãƒ æ€§ã®è€ƒæ…®**: LLM å‡ºåŠ›ã«ã¯ç¢ºç‡çš„ãªå¤‰å‹•ãŒã‚ã‚‹
2. **å¤–ã‚Œå€¤ã®æ¤œå‡º**: ä¸€æ™‚çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ãªã©ã®å½±éŸ¿ã‚’æ’é™¤
3. **ä¿¡é ¼åŒºé–“ã®è¨ˆç®—**: æ”¹å–„ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã‹ã‚’åˆ¤æ–­

### æ¨å¥¨å®Ÿè¡Œå›æ•°

| ãƒ•ã‚§ãƒ¼ã‚º | å®Ÿè¡Œå›æ•° | ç›®çš„ |
|---------|---------|------|
| **é–‹ç™ºä¸­** | 3å› | è¿…é€Ÿãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ |
| **è©•ä¾¡æ™‚** | 5å› | ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸä¿¡é ¼æ€§ |
| **æœ¬ç•ªå‰** | 10-20å› | é«˜ã„çµ±è¨ˆçš„ä¿¡é ¼æ€§ |

## ğŸ“Š çµ±è¨ˆåˆ†æ

### åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—

```python
import numpy as np
from scipy import stats

def statistical_analysis(
    baseline_results: List[float],
    improved_results: List[float],
    alpha: float = 0.05
) -> Dict:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ”¹å–„ç‰ˆã®çµ±è¨ˆçš„æ¯”è¼ƒ"""

    # åŸºæœ¬çµ±è¨ˆé‡
    baseline_stats = {
        "mean": np.mean(baseline_results),
        "std": np.std(baseline_results),
        "median": np.median(baseline_results),
        "min": np.min(baseline_results),
        "max": np.max(baseline_results)
    }

    improved_stats = {
        "mean": np.mean(improved_results),
        "std": np.std(improved_results),
        "median": np.median(improved_results),
        "min": np.min(improved_results),
        "max": np.max(improved_results)
    }

    # tæ¤œå®šï¼ˆå¯¾å¿œãªã—ï¼‰
    t_statistic, p_value = stats.ttest_ind(improved_results, baseline_results)

    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰
    pooled_std = np.sqrt(
        ((len(baseline_results) - 1) * baseline_stats["std"]**2 +
         (len(improved_results) - 1) * improved_stats["std"]**2) /
        (len(baseline_results) + len(improved_results) - 2)
    )
    cohens_d = (improved_stats["mean"] - baseline_stats["mean"]) / pooled_std

    # æ”¹å–„ç‡
    improvement_pct = (
        (improved_stats["mean"] - baseline_stats["mean"]) /
        baseline_stats["mean"] * 100
    )

    # ä¿¡é ¼åŒºé–“ï¼ˆ95%ï¼‰
    ci_baseline = stats.t.interval(
        0.95,
        len(baseline_results) - 1,
        loc=baseline_stats["mean"],
        scale=stats.sem(baseline_results)
    )

    ci_improved = stats.t.interval(
        0.95,
        len(improved_results) - 1,
        loc=improved_stats["mean"],
        scale=stats.sem(improved_results)
    )

    # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
    is_significant = p_value < alpha

    # åŠ¹æœã®å¤§ãã•ã®è§£é‡ˆ
    effect_size_interpretation = (
        "small" if abs(cohens_d) < 0.5 else
        "medium" if abs(cohens_d) < 0.8 else
        "large"
    )

    return {
        "baseline": baseline_stats,
        "improved": improved_stats,
        "comparison": {
            "improvement_pct": improvement_pct,
            "t_statistic": t_statistic,
            "p_value": p_value,
            "is_significant": is_significant,
            "cohens_d": cohens_d,
            "effect_size": effect_size_interpretation
        },
        "confidence_intervals": {
            "baseline": ci_baseline,
            "improved": ci_improved
        }
    }

# ä½¿ç”¨ä¾‹
baseline_accuracy = [73.0, 75.0, 77.0, 74.0, 76.0]  # 5å›ã®å®Ÿè¡Œçµæœ
improved_accuracy = [85.0, 87.0, 86.0, 88.0, 84.0]  # æ”¹å–„å¾Œã®5å›ã®å®Ÿè¡Œçµæœ

analysis = statistical_analysis(baseline_accuracy, improved_accuracy)
print(f"Improvement: {analysis['comparison']['improvement_pct']:.1f}%")
print(f"P-value: {analysis['comparison']['p_value']:.4f}")
print(f"Significant: {analysis['comparison']['is_significant']}")
print(f"Effect size: {analysis['comparison']['effect_size']}")
```

## ğŸ¯ çµ±è¨ˆçš„æœ‰æ„æ€§ã®è§£é‡ˆ

### På€¤ã®è§£é‡ˆ

| På€¤ | è§£é‡ˆ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|-----|------|-----------|
| p < 0.01 | **éå¸¸ã«æœ‰æ„** | æ”¹å–„ã‚’ç¢ºä¿¡ã—ã¦æ¡ç”¨ |
| p < 0.05 | **æœ‰æ„** | æ”¹å–„ã¨ã—ã¦æ¡ç”¨å¯èƒ½ |
| p < 0.10 | **ã‚„ã‚„æœ‰æ„** | è¿½åŠ æ¤œè¨¼ã‚’æ¤œè¨ |
| p â‰¥ 0.10 | **æœ‰æ„ã§ãªã„** | æ”¹å–„åŠ¹æœãªã—ã¨åˆ¤æ–­ |

### åŠ¹æœé‡ï¼ˆCohen's dï¼‰ã®è§£é‡ˆ

| Cohen's d | åŠ¹æœã®å¤§ãã• | æ„å‘³ |
|-----------|------------|------|
| d < 0.2 | **ç„¡è¦–ã§ãã‚‹** | å®Ÿè³ªçš„ãªæ”¹å–„ãªã— |
| 0.2 â‰¤ d < 0.5 | **å°** | ã‚ãšã‹ãªæ”¹å–„ |
| 0.5 â‰¤ d < 0.8 | **ä¸­** | æ˜ç¢ºãªæ”¹å–„ |
| d â‰¥ 0.8 | **å¤§** | å¤§å¹…ãªæ”¹å–„ |

## ğŸ“‰ å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨å‡¦ç†

### å¤–ã‚Œå€¤æ¤œå‡º

```python
def detect_outliers(data: List[float], method: str = "iqr") -> List[int]:
    """å¤–ã‚Œå€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œå‡º"""
    data_array = np.array(data)

    if method == "iqr":
        # IQRæ³•ï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰
        q1 = np.percentile(data_array, 25)
        q3 = np.percentile(data_array, 75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        outliers = [
            i for i, val in enumerate(data)
            if val < lower_bound or val > upper_bound
        ]

    elif method == "zscore":
        # Z-scoreæ³•
        mean = np.mean(data_array)
        std = np.std(data_array)
        z_scores = np.abs((data_array - mean) / std)

        outliers = [i for i, z in enumerate(z_scores) if z > 3]

    return outliers

# ä½¿ç”¨ä¾‹
results = [75.0, 76.0, 74.0, 77.0, 95.0]  # 95.0 ãŒå¤–ã‚Œå€¤ã®å¯èƒ½æ€§
outliers = detect_outliers(results, method="iqr")
print(f"Outlier indices: {outliers}")  # => [4]
```

### å¤–ã‚Œå€¤ã®å‡¦ç†æ–¹é‡

1. **èª¿æŸ»**: ãªãœå¤–ã‚Œå€¤ãŒç™ºç”Ÿã—ãŸã‹åŸå› ã‚’ç‰¹å®š
2. **é™¤å¤–åˆ¤æ–­**:
   - æ˜ã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ãªã©ï¼‰â†’ é™¤å¤–
   - å®Ÿéš›ã®æ€§èƒ½å¤‰å‹• â†’ ä¿æŒ
3. **è¨˜éŒ²**: å¤–ã‚Œå€¤ã®åŸå› ã¨å‡¦ç†ã‚’æ–‡æ›¸åŒ–

## ğŸ”„ åå¾©æ¸¬å®šã«ãŠã‘ã‚‹æ³¨æ„ç‚¹

### ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—

```python
from scipy.stats import ttest_ind_from_stats

def required_sample_size(
    baseline_mean: float,
    baseline_std: float,
    expected_improvement_pct: float,
    alpha: float = 0.05,
    power: float = 0.8
) -> int:
    """å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’æ¨å®š"""
    improved_mean = baseline_mean * (1 + expected_improvement_pct / 100)

    # åŠ¹æœé‡ã®è¨ˆç®—
    effect_size = abs(improved_mean - baseline_mean) / baseline_std

    # ç°¡æ˜“çš„ãªæ¨å®šï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ã¯ statsmodels.stats.power ã‚’ä½¿ç”¨ï¼‰
    if effect_size < 0.2:
        return 100  # å°ã•ã„åŠ¹æœã«ã¯å¤šãã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¿…è¦
    elif effect_size < 0.5:
        return 50
    elif effect_size < 0.8:
        return 30
    else:
        return 20

# ä½¿ç”¨ä¾‹
sample_size = required_sample_size(
    baseline_mean=75.0,
    baseline_std=3.0,
    expected_improvement_pct=10.0
)
print(f"Required sample size: {sample_size}")
```

## ğŸ“Š ä¿¡é ¼åŒºé–“ã®å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

def plot_confidence_intervals(
    baseline_results: List[float],
    improved_results: List[float],
    labels: List[str] = ["Baseline", "Improved"]
):
    """ä¿¡é ¼åŒºé–“ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    fig, ax = plt.subplots(figsize=(10, 6))

    # çµ±è¨ˆè¨ˆç®—
    baseline_mean = np.mean(baseline_results)
    baseline_ci = stats.t.interval(
        0.95,
        len(baseline_results) - 1,
        loc=baseline_mean,
        scale=stats.sem(baseline_results)
    )

    improved_mean = np.mean(improved_results)
    improved_ci = stats.t.interval(
        0.95,
        len(improved_results) - 1,
        loc=improved_mean,
        scale=stats.sem(improved_results)
    )

    # ãƒ—ãƒ­ãƒƒãƒˆ
    positions = [1, 2]
    means = [baseline_mean, improved_mean]
    cis = [
        (baseline_mean - baseline_ci[0], baseline_ci[1] - baseline_mean),
        (improved_mean - improved_ci[0], improved_ci[1] - improved_mean)
    ]

    ax.errorbar(positions, means, yerr=np.array(cis).T, fmt='o', markersize=10, capsize=10)
    ax.set_xticks(positions)
    ax.set_xticklabels(labels)
    ax.set_ylabel("Metric Value")
    ax.set_title("Comparison with 95% Confidence Intervals")
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("confidence_intervals.png")
    print("Plot saved: confidence_intervals.png")
```

## ğŸ“‹ çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
## çµ±è¨ˆçš„åˆ†æçµæœ

### åŸºæœ¬çµ±è¨ˆé‡

| æŒ‡æ¨™ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | æ”¹å–„ç‰ˆ | æ”¹å–„ |
|-----|------------|-------|------|
| å¹³å‡ | 75.0% | 86.0% | +11.0% |
| æ¨™æº–åå·® | 3.2% | 2.1% | -1.1% |
| ä¸­å¤®å€¤ | 75.0% | 86.0% | +11.0% |
| æœ€å°å€¤ | 70.0% | 84.0% | +14.0% |
| æœ€å¤§å€¤ | 80.0% | 88.0% | +8.0% |

### çµ±è¨ˆçš„æ¤œå®š

- **tçµ±è¨ˆé‡**: 8.45
- **På€¤**: 0.0001 (p < 0.01)
- **çµ±è¨ˆçš„æœ‰æ„æ€§**: âœ… éå¸¸ã«æœ‰æ„
- **åŠ¹æœé‡ï¼ˆCohen's dï¼‰**: 2.3 (large)

### ä¿¡é ¼åŒºé–“ï¼ˆ95%ï¼‰

- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: [72.8%, 77.2%]
- **æ”¹å–„ç‰ˆ**: [84.9%, 87.1%]

### çµè«–

æ”¹å–„ã¯çµ±è¨ˆçš„ã«éå¸¸ã«æœ‰æ„ã§ã‚ã‚Šï¼ˆp < 0.01ï¼‰ã€åŠ¹æœé‡ã‚‚å¤§ãã„ï¼ˆCohen's d = 2.3ï¼‰ã€‚
ä¿¡é ¼åŒºé–“ã®é‡ãªã‚Šã‚‚ãªãã€æ”¹å–„åŠ¹æœã¯ç¢ºå®Ÿã¨åˆ¤æ–­ã§ãã‚‹ã€‚
```

## ğŸ“‹ é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [è©•ä¾¡æŒ‡æ¨™](./evaluation_metrics.md) - æŒ‡æ¨™ã®å®šç¾©ã¨è¨ˆç®—æ–¹æ³•
- [ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ](./evaluation_testcases.md) - ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ§‹é€ 
- [ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](./evaluation_practices.md) - è©•ä¾¡ã®å®Ÿè·µçš„ãªã‚¬ã‚¤ãƒ‰
