# Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–

æœ€çµ‚çš„ãªæˆæœã‚’è¨˜éŒ²ã—ã€ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã€‚

**æ‰€è¦æ™‚é–“**: 30åˆ†-1æ™‚é–“

**ğŸ“‹ é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“](./workflow.md) | [å®Ÿè·µä¾‹](./examples.md)

---

## Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–

### Step 12: æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

**ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```markdown
# LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå]
å®Ÿæ–½æœŸé–“: 2024-11-24 10:00 - 2024-11-24 15:00 (5æ™‚é–“)
å®Ÿæ–½è€…: Claude Code with fine-tune skill

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

æœ¬ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’å®Ÿæ–½ã—ã€ä»¥ä¸‹ã®æˆæœã‚’é”æˆã—ã¾ã—ãŸï¼š

- âœ… **Accuracy**: 75.0% â†’ 92.0% (+17.0%, ç›®æ¨™90%é”æˆ)
- âœ… **Latency**: 2.5s â†’ 1.9s (-24.0%, ç›®æ¨™2.0sé”æˆ)
- âš ï¸ **Cost**: $0.015 â†’ $0.011 (-26.7%, ç›®æ¨™$0.010æœªé”)

å…¨3 iterations ã‚’å®Ÿæ–½ã—ã€3ã¤ã®æŒ‡æ¨™ã®ã†ã¡2ã¤ã§ç›®æ¨™ã‚’é”æˆã—ã¾ã—ãŸã€‚

## å®Ÿæ–½å†…å®¹ã‚µãƒãƒªãƒ¼

### Iteration æ•°ã¨å®Ÿè¡Œæ™‚é–“
- **Total Iterations**: 3
- **æœ€é©åŒ–ã—ãŸãƒãƒ¼ãƒ‰æ•°**: 2 (analyze_intent, generate_response)
- **è©•ä¾¡å®Ÿè¡Œå›æ•°**: 20å› (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³5å› + å„iterationå¾Œ5å›Ã—3)
- **ç·å®Ÿè¡Œæ™‚é–“**: ç´„5æ™‚é–“

### æœ€çµ‚çµæœ

| æŒ‡æ¨™ | åˆæœŸå€¤ | æœ€çµ‚å€¤ | æ”¹å–„ | æ”¹å–„ç‡ | ç›®æ¨™ | é”æˆçŠ¶æ³ |
|------|--------|--------|------|--------|------|---------|
| Accuracy | 75.0% | 92.0% | +17.0% | +22.7% | 90.0% | âœ… 102.2% é”æˆ |
| Latency | 2.5s | 1.9s | -0.6s | -24.0% | 2.0s | âœ… 95.0% é”æˆ |
| Cost/req | $0.015 | $0.011 | -$0.004 | -26.7% | $0.010 | âš ï¸ 90.9% é”æˆ |

## Iteration åˆ¥ã®è©³ç´°

### Iteration 1: analyze_intent ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 11:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. temperature: 1.0 â†’ 0.3
2. Few-shot examples ã‚’5å€‹è¿½åŠ 
3. JSON å‡ºåŠ›å½¢å¼ã«æ§‹é€ åŒ–
4. æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆ4ã¤ï¼‰ã‚’å®šç¾©

**çµæœ**:
- Accuracy: 75.0% â†’ 86.0% (+11.0%)
- Latency: 2.5s â†’ 2.4s (-0.1s)
- Cost: $0.015 â†’ $0.014 (-$0.001)

**å­¦ã³**: Few-shot examples ã¨æ˜ç¢ºãªå‡ºåŠ›å½¢å¼ãŒ accuracy å‘ä¸Šã«æœ€ã‚‚åŠ¹æœçš„

---

### Iteration 2: generate_response ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 13:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/generator.py:45-68

**å¤‰æ›´å†…å®¹**:
1. ç°¡æ½”æ€§ã®æŒ‡ç¤ºã‚’è¿½åŠ ï¼ˆ"2-3æ–‡ã§å›ç­”"ï¼‰
2. max_tokens: unlimited â†’ 500
3. temperature: 0.7 â†’ 0.5
4. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ˜ç¢ºåŒ–

**çµæœ**:
- Accuracy: 86.0% â†’ 88.0% (+2.0%)
- Latency: 2.4s â†’ 2.0s (-0.4s)
- Cost: $0.014 â†’ $0.011 (-$0.003)

**å­¦ã³**: max_tokens åˆ¶é™ãŒ latency ã¨ cost å‰Šæ¸›ã«å¤§ããè²¢çŒ®

---

### Iteration 3: analyze_intent ã®è¿½åŠ æ”¹å–„

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 14:30
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. Few-shot examples ã‚’ 5å€‹ â†’ 10å€‹ã«å¢—åŠ 
2. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ 
3. confidence threshold ã«ã‚ˆã‚‹å†åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯

**çµæœ**:
- Accuracy: 88.0% â†’ 92.0% (+4.0%)
- Latency: 2.0s â†’ 1.9s (-0.1s)
- Cost: $0.011 â†’ $0.011 (Â±0)

**å­¦ã³**: è¿½åŠ ã® few-shot examples ãŒ accuracy ã®æœ€å¾Œã®å£ã‚’çªç ´

## æœ€çµ‚çš„ãªå¤‰æ›´å†…å®¹

### src/nodes/analyzer.py (analyze_intent ãƒãƒ¼ãƒ‰)

#### Before
```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=1.0)
    messages = [
        SystemMessage(content="You are an intent analyzer. Analyze user input."),
        HumanMessage(content=f"Analyze: {state['user_input']}")
    ]
    response = llm.invoke(messages)
    state["intent"] = response.content
    return state
```

#### After
```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0.3)

    system_prompt = """You are an intent classifier for a customer support chatbot.
Classify user input into: product_inquiry, technical_support, billing, or general.
Output JSON: {"intent": "<category>", "confidence": <0.0-1.0>, "reasoning": "<explanation>"}

[10 few-shot examples...]
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Input: {state['user_input']}\nOutput:")
    ]

    response = llm.invoke(messages)
    intent_data = json.loads(response.content)

    # Low confidence â†’ re-classify as general
    if intent_data["confidence"] < 0.7:
        intent_data["intent"] = "general"

    state["intent"] = intent_data["intent"]
    state["confidence"] = intent_data["confidence"]
    return state
```

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 1.0 â†’ 0.3
- Few-shot examples: 0 â†’ 10
- å‡ºåŠ›: è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆ â†’ JSON
- Confidence threshold ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 

---

### src/nodes/generator.py (generate_response ãƒãƒ¼ãƒ‰)

#### Before
```python
def generate_response(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0.7)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Generate helpful response based on context."),
        ("human", "{context}\n\nQuestion: {question}")
    ])
    chain = prompt | llm
    response = chain.invoke({"context": state["context"], "question": state["user_input"]})
    state["response"] = response.content
    return state
```

#### After
```python
def generate_response(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0.5,
        max_tokens=500  # å‡ºåŠ›é•·åˆ¶é™
    )

    system_prompt = """You are a helpful customer support assistant.

Guidelines:
- Be concise: Answer in 2-3 sentences
- Be friendly: Use a warm, professional tone
- Be accurate: Base your answer on the provided context
- If uncertain: Acknowledge and offer to escalate

Format: Direct answer followed by one optional clarifying sentence.
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Context: {context}\n\nQuestion: {question}\n\nAnswer:")
    ])

    chain = prompt | llm
    response = chain.invoke({"context": state["context"], "question": state["user_input"]})
    state["response"] = response.content
    return state
```

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 0.7 â†’ 0.5
- max_tokens: unlimited â†’ 500
- ç°¡æ½”æ€§ã®æ˜ç¢ºãªæŒ‡ç¤ºï¼ˆ"2-3 sentences"ï¼‰
- å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³è¿½åŠ 

## è©•ä¾¡çµæœã®è©³ç´°

### Test Case åˆ¥ã®æ”¹å–„çŠ¶æ³

| Case ID | Category | Before | After | æ”¹å–„ |
|---------|----------|--------|-------|------|
| TC001 | Product | âŒ Wrong | âœ… Correct | âœ… |
| TC002 | Technical | âŒ Wrong | âœ… Correct | âœ… |
| TC003 | Billing | âœ… Correct | âœ… Correct | - |
| TC004 | General | âœ… Correct | âœ… Correct | - |
| TC005 | Product | âŒ Wrong | âœ… Correct | âœ… |
| ... | ... | ... | ... | ... |
| TC020 | Technical | âœ… Correct | âœ… Correct | - |

**æ”¹å–„ã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 15/20 (75%)
**ç¶­æŒã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 5/20 (25%)
**åŠ£åŒ–ã—ãŸã‚±ãƒ¼ã‚¹**: 0/20 (0%)

### Latency ã®å†…è¨³

| ãƒãƒ¼ãƒ‰ | Before | After | å¤‰åŒ– | å¤‰åŒ–ç‡ |
|--------|--------|-------|------|--------|
| analyze_intent | 0.5s | 0.4s | -0.1s | -20% |
| retrieve_context | 0.2s | 0.2s | Â±0s | 0% |
| generate_response | 1.8s | 1.3s | -0.5s | -28% |
| **Total** | **2.5s** | **1.9s** | **-0.6s** | **-24%** |

### Cost ã®å†…è¨³

| ãƒãƒ¼ãƒ‰ | Before | After | å¤‰åŒ– | å¤‰åŒ–ç‡ |
|--------|--------|-------|------|--------|
| analyze_intent | $0.003 | $0.003 | Â±$0 | 0% |
| retrieve_context | $0.001 | $0.001 | Â±$0 | 0% |
| generate_response | $0.011 | $0.007 | -$0.004 | -36% |
| **Total** | **$0.015** | **$0.011** | **-$0.004** | **-27%** |

## ä»Šå¾Œã®æ¨å¥¨äº‹é …

### çŸ­æœŸï¼ˆ1-2é€±é–“ï¼‰
1. **Cost ç›®æ¨™ã®é”æˆ**: $0.011 â†’ $0.010
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Claude 3.5 Haiku ã¸ã®éƒ¨åˆ†ç§»è¡Œã‚’æ¤œè¨
   - æ¨å®šåŠ¹æœ: -$0.002-0.003/req

2. **Accuracy ã®æ›´ãªã‚‹å‘ä¸Š**: 92.0% â†’ 95.0%
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®åˆ†æã¨ few-shot examples ã®è¿½åŠ 
   - æ¨å®šåŠ¹æœ: +3.0%

### ä¸­æœŸï¼ˆ1-2ãƒ¶æœˆï¼‰
1. **ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–**
   - simple ãª intent classification ã«ã¯ Haiku ã‚’ä½¿ç”¨
   - complex ãª response generation ã®ã¿ Sonnet ã‚’ä½¿ç”¨
   - æ¨å®šåŠ¹æœ: -30-40% cost, latency ã¸ã®å½±éŸ¿ã¯æœ€å°

2. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ´»ç”¨**
   - System prompts ã¨ few-shot examples ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - æ¨å®šåŠ¹æœ: -50% costï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰

### é•·æœŸï¼ˆ3-6ãƒ¶æœˆï¼‰
1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨**
   - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ã® model fine-tuning
   - Few-shot examples ä¸è¦ã§ç°¡æ½”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
   - æ¨å®šåŠ¹æœ: -60% cost, +5% accuracy

## çµè«–

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸï¼š

âœ… **æˆåŠŸã—ãŸç‚¹**:
1. Accuracy ã®å¤§å¹…å‘ä¸Šï¼ˆ+22.7%ï¼‰- ç›®æ¨™ã‚’2.2%è¶…éé”æˆ
2. Latency ã®é¡•è‘—ãªæ”¹å–„ï¼ˆ-24.0%ï¼‰- ç›®æ¨™ã‚’5%è¶…éé”æˆ
3. Cost ã®å‰Šæ¸›ï¼ˆ-26.7%ï¼‰- ç›®æ¨™ã«ã‚ã¨9.1%

âš ï¸ **èª²é¡Œ**:
1. Cost ç›®æ¨™æœªé”ï¼ˆ$0.011 vs $0.010ç›®æ¨™ï¼‰- è»½é‡ãƒ¢ãƒ‡ãƒ«ã¸ã®ç§»è¡Œã§å¯¾å¿œå¯èƒ½

ğŸ“ˆ **ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã®å‘ä¸Šï¼ˆaccuracyå‘ä¸Šã«ã‚ˆã‚Šï¼‰
- é‹ç”¨ã‚³ã‚¹ãƒˆã®å‰Šæ¸›ï¼ˆlatency, costå‰Šæ¸›ã«ã‚ˆã‚Šï¼‰
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ”¹å–„ï¼ˆåŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ï¼‰

ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
1. Cost å‰Šæ¸›ã®ãŸã‚ã®è»½é‡ãƒ¢ãƒ‡ãƒ«ç§»è¡Œã®æ¤œè¨¼
2. ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡
3. æ–°ã—ã„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¸ã®å±•é–‹

---

ä½œæˆæ—¥æ™‚: 2024-11-24 15:00:00
ä½œæˆè€…: Claude Code (fine-tune skill)
```

### Step 13: ã‚³ãƒ¼ãƒ‰ã‚³ãƒŸãƒƒãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

**Git ã‚³ãƒŸãƒƒãƒˆä¾‹**:
```bash
# å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
git add src/nodes/analyzer.py src/nodes/generator.py
git commit -m "feat: optimize LangGraph prompts for accuracy and latency

Iteration 1-3 of fine-tuning process:
- analyze_intent: added few-shot examples, JSON output, lower temperature
- generate_response: added conciseness guidelines, max_tokens limit

Results:
- Accuracy: 75.0% â†’ 92.0% (+17.0%, goal 90% âœ…)
- Latency: 2.5s â†’ 1.9s (-0.6s, goal 2.0s âœ…)
- Cost: $0.015 â†’ $0.011 (-$0.004, goal $0.010 âš ï¸)

Full report: evaluation_results/final_report.md"

# è©•ä¾¡çµæœã‚‚ã‚³ãƒŸãƒƒãƒˆ
git add evaluation_results/
git commit -m "docs: add fine-tuning evaluation results and final report"

# ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹
git tag -a fine-tune-v1.0 -m "Fine-tuning completed: 92% accuracy achieved"
```

## ã¾ã¨ã‚

ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¾“ã†ã“ã¨ã§ï¼š
- âœ… ä½“ç³»çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
- âœ… ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®æ„æ€æ±ºå®š
- âœ… ç¶™ç¶šçš„ãªæ”¹å–„ã¨æ¤œè¨¼
- âœ… å®Œå…¨ãªæ–‡æ›¸åŒ–ã¨ãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£

ãŒå®Ÿç¾ã§ãã¾ã™ã€‚
