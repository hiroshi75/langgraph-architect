# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

LangGraph ãƒãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åŠ¹æœçš„ã«æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®å®Ÿè·µçš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯é›†ã€‚

**ğŸ’¡ Tip**: æ”¹å–„å‰å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¯”è¼ƒä¾‹ã¨ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ [examples.md](examples.md#phase-3-åå¾©çš„æ”¹å–„ã®ä¾‹) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ”§ å®Ÿè·µçš„ãªæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 1: Few-Shot Examplesï¼ˆå°‘æ•°ä¾‹å­¦ç¿’ï¼‰

**åŠ¹æœ**: Accuracy +10-20%

**Beforeï¼ˆZero-shotï¼‰**:
```python
system_prompt = """Classify user input into: product_inquiry, technical_support, billing, or general."""

# Accuracy: ~70%
```

**Afterï¼ˆFew-shotï¼‰**:
```python
system_prompt = """Classify user input into: product_inquiry, technical_support, billing, or general.

Examples:

Input: "How much does the premium plan cost?"
Output: product_inquiry

Input: "I can't log into my account"
Output: technical_support

Input: "Why was I charged twice this month?"
Output: billing

Input: "Hello, how are you today?"
Output: general

Input: "What features are included in the basic plan?"
Output: product_inquiry"""

# Accuracy: ~85-90%
```

**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**:
- **Examples æ•°**: 3-7å€‹ï¼ˆãã‚Œä»¥ä¸Šã¯åç›Šé€“æ¸›ï¼‰
- **å¤šæ§˜æ€§**: å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æœ€ä½1å€‹ã€edge cases ã‚’å«ã‚€
- **å“è³ª**: æ˜ç¢ºã§è­°è«–ã®ä½™åœ°ã®ãªã„ä¾‹ã‚’é¸æŠ
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: ä¸€è²«ã—ãŸ Input/Output å½¢å¼

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 2: Chain-of-Thoughtï¼ˆæ€è€ƒã®é€£é–ï¼‰

**åŠ¹æœ**: Complex reasoning tasks ã§ Accuracy +15-30%

**Beforeï¼ˆDirect answerï¼‰**:
```python
prompt = f"""Question: {question}

Answer:"""

# è¤‡é›‘ãªè³ªå•ã§èª¤ç­”ãŒå¤šã„
```

**Afterï¼ˆChain-of-Thoughtï¼‰**:
```python
prompt = f"""Question: {question}

Think through this step by step:

1. First, identify the key information needed
2. Then, analyze the context for relevant details
3. Finally, formulate a clear answer

Reasoning:"""

# è¤‡é›‘ãªè³ªå•ã§ã‚‚è«–ç†çš„ã«å›ç­”
```

**é©ç”¨ã‚·ãƒŠãƒªã‚ª**:
- âœ… è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®æ¨è«–ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯
- âœ… è¤‡é›‘ãªæ„æ€æ±ºå®š
- âœ… çŸ›ç›¾ã®è§£æ±º
- âŒ ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡ã‚¿ã‚¹ã‚¯ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 3: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ§‹é€ åŒ–

**åŠ¹æœ**: Latency -10-20%, Parsing errors -90%

**Beforeï¼ˆè‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆï¼‰**:
```python
prompt = "Classify the intent and explain why."

# Output: "This looks like a technical support question because the user is having trouble logging in..."
# å•é¡Œ: ãƒ‘ãƒ¼ã‚¹ãŒé›£ã—ã„ã€å†—é•·ã€ä¸€è²«æ€§ãŒãªã„
```

**Afterï¼ˆJSON æ§‹é€ åŒ–ï¼‰**:
```python
prompt = """Classify the intent.

Output ONLY a valid JSON object:
{
  "intent": "<category>",
  "confidence": <0.0-1.0>,
  "reasoning": "<brief explanation in one sentence>"
}

Example output:
{"intent": "technical_support", "confidence": 0.95, "reasoning": "User reports authentication issue"}"""

# Output: {"intent": "technical_support", "confidence": 0.95, "reasoning": "User reports authentication issue"}
# åˆ©ç‚¹: ç°¡å˜ã«ãƒ‘ãƒ¼ã‚¹ã€ç°¡æ½”ã€ä¸€è²«æ€§
```

**JSON ãƒ‘ãƒ¼ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**:
```python
import json
import re

def parse_llm_json_output(output: str) -> dict:
    """LLM ã® JSON å‡ºåŠ›ã‚’å …ç‰¢ã«ãƒ‘ãƒ¼ã‚¹"""
    try:
        # ãã®ã¾ã¾ JSON ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        return json.loads(output)
    except json.JSONDecodeError:
        # JSON ã®ã¿ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã©ã‹ã‚‰ï¼‰
        json_match = re.search(r'\{[^}]+\}', output)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return {
            "intent": "general",
            "confidence": 0.5,
            "reasoning": "Failed to parse LLM output"
        }
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 4: Temperature ã¨ Max Tokens ã®èª¿æ•´

**Temperature ã®åŠ¹æœ**:

| ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— | æ¨å¥¨ Temperature | ç†ç”± |
|------------|-----------------|------|
| åˆ†é¡ãƒ»æŠ½å‡º | 0.0 - 0.3 | æ±ºå®šè«–çš„ãªå‡ºåŠ›ãŒæœ›ã¾ã—ã„ |
| è¦ç´„ãƒ»å¤‰æ› | 0.3 - 0.5 | ä¸€å®šã®æŸ”è»Ÿæ€§ãŒå¿…è¦ |
| å‰µä½œãƒ»ç”Ÿæˆ | 0.7 - 1.0 | å¤šæ§˜æ€§ã¨å‰µé€ æ€§ãŒé‡è¦ |

**Beforeï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰**:
```python
llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€å…¨ã‚¿ã‚¹ã‚¯ã§ä½¿ç”¨
)
# åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ä¸å®‰å®šãªçµæœ
```

**Afterï¼ˆã‚¿ã‚¹ã‚¯ã”ã¨ã«æœ€é©åŒ–ï¼‰**:
```python
# Intent classification: ä½ã„ temperature
intent_llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.3  # ä¸€è²«æ€§ã‚’é‡è¦–
)

# Response generation: ä¸­ç¨‹åº¦ã® temperature
response_llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.5,  # æŸ”è»Ÿæ€§ã¨ãƒãƒ©ãƒ³ã‚¹
    max_tokens=500    # ç°¡æ½”ã•ã‚’å¼·åˆ¶
)
```

**Max Tokens ã®åŠ¹æœ**:

```python
# Before: åˆ¶é™ãªã—
llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")
# Average output: 800 tokens, Cost: $0.012/req, Latency: 3.2s

# After: é©åˆ‡ãªåˆ¶é™
llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    max_tokens=500  # å¿…è¦ååˆ†ãªé•·ã•
)
# Average output: 450 tokens, Cost: $0.007/req (-42%), Latency: 1.8s (-44%)
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 5: System Message vs Human Message ã®ä½¿ã„åˆ†ã‘

**System Messageï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰**:
- **ç”¨é€”**: å½¹å‰²ã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€åˆ¶ç´„
- **ç‰¹å¾´**: ã‚¿ã‚¹ã‚¯å…¨ä½“ã«é©ç”¨ã•ã‚Œã‚‹æ–‡è„ˆ
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: åŠ¹æœçš„ï¼ˆé »ç¹ã«å¤‰ã‚ã‚‰ãªã„ï¼‰

**Human Messageï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰**:
- **ç”¨é€”**: å…·ä½“çš„ãªå…¥åŠ›ã€è³ªå•
- **ç‰¹å¾´**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«å¤‰ã‚ã‚‹
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: åŠ¹æœãŒä½ã„

**è‰¯ã„æ§‹é€ **:
```python
messages = [
    SystemMessage(content="""You are a customer support assistant.

Guidelines:
- Be concise: 2-3 sentences maximum
- Be empathetic: Acknowledge customer concerns
- Be actionable: Provide clear next steps

Response format:
1. Acknowledgment
2. Answer or solution
3. Next steps (if applicable)"""),

    HumanMessage(content=f"""Customer question: {user_input}

Context: {context}

Generate a helpful response:""")
]
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 6: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆPrompt Cachingï¼‰

**åŠ¹æœ**: Cost -50-90%ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰

Anthropic Claude ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚’æ´»ç”¨ï¼š

```python
from anthropic import Anthropic

client = Anthropic()

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯èƒ½ãªå¤§ããª system prompt
CACHED_SYSTEM_PROMPT = """You are an expert customer support agent...

[Long guidelines, examples, and context - 1000+ tokens]

Examples:
[50 few-shot examples]
"""

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=500,
    system=[
        {
            "type": "text",
            "text": CACHED_SYSTEM_PROMPT,
            "cache_control": {"type": "ephemeral"}  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
        }
    ],
    messages=[
        {"role": "user", "content": user_input}
    ]
)

# åˆå›: ãƒ•ãƒ«ã‚³ã‚¹ãƒˆ
# 2å›ç›®ä»¥é™ï¼ˆ5åˆ†ä»¥å†…ï¼‰: Input tokens ãŒ -90% å‰²å¼•
```

**ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥**:
- âœ… å¤§ããª system promptsï¼ˆ>1024 tokensï¼‰
- âœ… Few-shot examples ã®é›†åˆ
- âœ… é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆRAG ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
- âŒ é »ç¹ã«å¤‰ã‚ã‚‹å†…å®¹
- âŒ å°ã•ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ<1024 tokensï¼‰

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 7: æ®µéšçš„ãªè©³ç´°åŒ–ï¼ˆProgressive Refinementï¼‰

è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’è¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ï¼š

**Beforeï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ï¼‰**:
```python
# 1ã¤ã®ãƒãƒ¼ãƒ‰ã§å…¨ã¦ã‚’å®Ÿè¡Œ
prompt = f"""Analyze user input, retrieve relevant info, and generate response.

Input: {user_input}"""

# å•é¡Œ: è¤‡é›‘ã™ãã¦å“è³ªãŒä½ã„ã€ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£
```

**Afterï¼ˆè¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ï¼‰**:
```python
# Step 1: Intent classification
intent = classify_intent(user_input)

# Step 2: Information retrieval (intent ã«åŸºã¥ã)
context = retrieve_context(intent, user_input)

# Step 3: Response generation (intent ã¨ context ã‚’ä½¿ç”¨)
response = generate_response(intent, context, user_input)

# åˆ©ç‚¹: å„ã‚¹ãƒ†ãƒƒãƒ—ãŒæœ€é©åŒ–å¯èƒ½ã€ãƒ‡ãƒãƒƒã‚°ãŒå®¹æ˜“ã€å“è³ªãŒå‘ä¸Š
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 8: Negative Instructionsï¼ˆç¦æ­¢äº‹é …ã®æ˜ç¤ºï¼‰

**åŠ¹æœ**: ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ã‚¨ãƒ©ãƒ¼ -30-50%

```python
prompt = """Generate a customer support response.

DO:
- Be concise (2-3 sentences)
- Acknowledge the customer's concern
- Provide actionable next steps

DO NOT:
- Apologize excessively (one apology maximum)
- Make promises you can't keep (e.g., "immediate resolution")
- Use technical jargon without explanation
- Provide information not in the context
- Generate placeholder text like "XXX" or "[insert here]"

Customer question: {question}
Context: {context}

Response:"""
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 9: Self-Consistencyï¼ˆè‡ªå·±ä¸€è²«æ€§ï¼‰

**åŠ¹æœ**: Complex reasoning ã§ Accuracy +10-20%ã€Cost +200-300%

è¤‡æ•°ã®æ¨è«–ãƒ‘ã‚¹ã‚’ç”Ÿæˆã—ã¦å¤šæ•°æ±ºï¼š

```python
def self_consistency_reasoning(question: str, num_samples: int = 5) -> str:
    """è¤‡æ•°ã®æ¨è«–ã‚’ç”Ÿæˆã—ã¦æœ€ã‚‚ä¸€è²«ã—ãŸç­”ãˆã‚’é¸æŠ"""

    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0.7  # å¤šæ§˜æ€§ã®ãŸã‚ã«é«˜ã‚ã® temperature
    )

    prompt = f"""Question: {question}

Think through this step by step and provide your reasoning:

Reasoning:"""

    # è¤‡æ•°ã®æ¨è«–ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
    responses = []
    for _ in range(num_samples):
        response = llm.invoke([HumanMessage(content=prompt)])
        responses.append(response.content)

    # æœ€ã‚‚ä¸€è²«ã—ãŸç­”ãˆã‚’æŠ½å‡ºï¼ˆç°¡ç•¥åŒ–ï¼‰
    # å®Ÿéš›ã«ã¯ã€å„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æœ€çµ‚çš„ãªç­”ãˆã‚’æŠ½å‡ºã—ã¦å¤šæ•°æ±º
    from collections import Counter
    final_answers = [extract_final_answer(r) for r in responses]
    most_common = Counter(final_answers).most_common(1)[0][0]

    return most_common

# ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:
# - Accuracy: +10-20%
# - Cost: +200-300% (5x API calls)
# - Latency: +200-300% (ä¸¦åˆ—åŒ–ã—ãªã„å ´åˆ)
# ä½¿ç”¨: Critical decisions ã®ã¿
```

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ 10: ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

**ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘åº¦ã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«é¸æŠ**:

| ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— | æ¨å¥¨ãƒ¢ãƒ‡ãƒ« | ç†ç”± |
|------------|-----------|------|
| Simple classification | Claude 3.5 Haiku | é«˜é€Ÿã€ä½ã‚³ã‚¹ãƒˆã€ååˆ†ãªç²¾åº¦ |
| Complex reasoning | Claude 3.5 Sonnet | ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½ |
| Highly complex tasks | Claude Opus | æœ€é«˜ã®æ€§èƒ½ï¼ˆã‚³ã‚¹ãƒˆé«˜ï¼‰ |

```python
# ã‚¿ã‚¹ã‚¯ã”ã¨ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
class LLMSelector:
    def __init__(self):
        self.haiku = ChatAnthropic(model="claude-3-5-haiku-20241022")
        self.sonnet = ChatAnthropic(model="claude-3-5-sonnet-20241022")
        self.opus = ChatAnthropic(model="claude-opus-20240229")

    def get_llm(self, task_complexity: str):
        if task_complexity == "simple":
            return self.haiku  # $0.001/req ç›¸å½“
        elif task_complexity == "complex":
            return self.sonnet  # $0.005/req ç›¸å½“
        else:  # very_complex
            return self.opus  # $0.015/req ç›¸å½“

# ä½¿ç”¨ä¾‹
selector = LLMSelector()

# Simple intent classification â†’ Haiku
intent_llm = selector.get_llm("simple")

# Complex response generation â†’ Sonnet
response_llm = selector.get_llm("complex")
```

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
```python
def hybrid_classification(user_input: str) -> dict:
    """ã¾ãš Haiku ã§è©¦ã—ã€confidence ãŒä½ã‘ã‚Œã° Sonnet ã‚’ä½¿ç”¨"""

    # Step 1: Haiku ã§åˆ†é¡
    haiku_result = classify_with_haiku(user_input)

    if haiku_result["confidence"] >= 0.8:
        # é«˜ã„ confidence â†’ Haiku ã®çµæœã‚’ä½¿ç”¨
        return haiku_result
    else:
        # ä½ã„ confidence â†’ Sonnet ã§å†åˆ†é¡
        sonnet_result = classify_with_sonnet(user_input)
        return sonnet_result

# åŠ¹æœ:
# - 80%ã®ã‚±ãƒ¼ã‚¹ã§ Haiku ã‚’ä½¿ç”¨ï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰
# - 20%ã®ã‚±ãƒ¼ã‚¹ã§ Sonnet ã‚’ä½¿ç”¨ï¼ˆé«˜ç²¾åº¦ï¼‰
# - å¹³å‡ã‚³ã‚¹ãƒˆ: -60%
# - å¹³å‡ç²¾åº¦: -2%ï¼ˆè¨±å®¹ç¯„å›²ï¼‰
```
