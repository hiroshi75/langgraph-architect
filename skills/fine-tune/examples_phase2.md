# Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹

è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨çµæœãƒ¬ãƒãƒ¼ãƒˆã®ä¾‹ã€‚

**ğŸ“‹ é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [å®Ÿè·µä¾‹ãƒˆãƒƒãƒ—](./examples.md) | [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ Phase 2](./workflow_phase2.md) | [è©•ä¾¡æ–¹æ³•](./evaluation.md)

---

## Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹

### Example 2.1: è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/evaluation/evaluator.py`

```python
import json
import time
from pathlib import Path
from typing import Dict, List

def evaluate_test_cases(test_cases: List[Dict]) -> Dict:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è©•ä¾¡"""
    results = {
        "total_cases": len(test_cases),
        "correct": 0,
        "total_latency": 0.0,
        "total_cost": 0.0,
        "case_results": []
    }

    for case in test_cases:
        start_time = time.time()

        # LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        output = run_langgraph_app(case["input"])

        latency = time.time() - start_time

        # æ­£è§£åˆ¤å®š
        is_correct = output["answer"] == case["expected_answer"]
        if is_correct:
            results["correct"] += 1

        # ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ï¼‰
        cost = calculate_cost(output["token_usage"])

        results["total_latency"] += latency
        results["total_cost"] += cost

        results["case_results"].append({
            "case_id": case["id"],
            "correct": is_correct,
            "latency": latency,
            "cost": cost
        })

    # æŒ‡æ¨™ã®è¨ˆç®—
    results["accuracy"] = (results["correct"] / results["total_cases"]) * 100
    results["avg_latency"] = results["total_latency"] / results["total_cases"]
    results["avg_cost"] = results["total_cost"] / results["total_cases"]

    return results

def calculate_cost(token_usage: Dict) -> float:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
    # Claude 3.5 Sonnet ã®æ–™é‡‘
    INPUT_COST_PER_1M = 3.0  # $3.00 per 1M input tokens
    OUTPUT_COST_PER_1M = 15.0  # $15.00 per 1M output tokens

    input_cost = (token_usage["input_tokens"] / 1_000_000) * INPUT_COST_PER_1M
    output_cost = (token_usage["output_tokens"] / 1_000_000) * OUTPUT_COST_PER_1M

    return input_cost + output_cost

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    with open("tests/evaluation/test_cases.json") as f:
        test_cases = json.load(f)["test_cases"]

    # è©•ä¾¡å®Ÿè¡Œ
    results = evaluate_test_cases(test_cases)

    # çµæœã‚’ä¿å­˜
    with open("evaluation_results/baseline_run.json", "w") as f:
        json.dump(results, f, indent=2)

    print(f"Accuracy: {results['accuracy']:.1f}%")
    print(f"Avg Latency: {results['avg_latency']:.2f}s")
    print(f"Avg Cost: ${results['avg_cost']:.4f}")
```

### Example 2.2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `scripts/baseline_evaluation.sh`

```bash
#!/bin/bash

ITERATIONS=5
RESULTS_DIR="evaluation_results/baseline"
mkdir -p $RESULTS_DIR

echo "Starting baseline evaluation: $ITERATIONS iterations"

for i in $(seq 1 $ITERATIONS); do
    echo "----------------------------------------"
    echo "Iteration $i/$ITERATIONS"
    echo "----------------------------------------"

    uv run python -m tests.evaluation.evaluator \
        --output "$RESULTS_DIR/run_$i.json" \
        --verbose

    echo "Completed iteration $i"

    # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    if [ $i -lt $ITERATIONS ]; then
        echo "Waiting 5 seconds before next iteration..."
        sleep 5
    fi
done

echo ""
echo "All iterations completed. Aggregating results..."

# çµæœã®é›†è¨ˆ
uv run python -m tests.evaluation.aggregate \
    --input-dir "$RESULTS_DIR" \
    --output "$RESULTS_DIR/summary.json"

echo "Baseline evaluation complete!"
echo "Results saved to: $RESULTS_DIR/summary.json"
```

### Example 2.3: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœãƒ¬ãƒãƒ¼ãƒˆ

```markdown
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡çµæœ

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 10:00:00
å®Ÿè¡Œå›æ•°: 5
ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: 20

## è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™     | å¹³å‡   | æ¨™æº–åå·® | æœ€å°å€¤ | æœ€å¤§å€¤ | ç›®æ¨™   | ã‚®ãƒ£ãƒƒãƒ—  |
| -------- | ------ | -------- | ------ | ------ | ------ | --------- |
| Accuracy | 75.0%  | 3.2%     | 70.0%  | 80.0%  | 90.0%  | **-15.0%** |
| Latency  | 2.5s   | 0.4s     | 2.1s   | 3.2s   | 2.0s   | **+0.5s**  |
| Cost/req | $0.015 | $0.002   | $0.013 | $0.018 | $0.010 | **+$0.005** |

## è©³ç´°åˆ†æ

### Accuracy ã®å•é¡Œ

- **ç¾çŠ¶**: 75.0% (ç›®æ¨™: 90.0%)
- **ä¸»ãªèª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³**:
  1. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ãƒŸã‚¹: 12 ã‚±ãƒ¼ã‚¹ (60%ã®èª¤ç­”)
  2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ä¸è¶³: 5 ã‚±ãƒ¼ã‚¹ (25%ã®èª¤ç­”)
  3. æ›–æ˜§ãªè³ªå•ã¸ã®å¯¾å¿œ: 3 ã‚±ãƒ¼ã‚¹ (15%ã®èª¤ç­”)

### Latency ã®å•é¡Œ

- **ç¾çŠ¶**: 2.5s (ç›®æ¨™: 2.0s)
- **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
  1. generate_response ãƒãƒ¼ãƒ‰: å¹³å‡ 1.8s (å…¨ä½“ã® 72%)
  2. analyze_intent ãƒãƒ¼ãƒ‰: å¹³å‡ 0.5s (å…¨ä½“ã® 20%)
  3. ãã®ä»–: å¹³å‡ 0.2s (å…¨ä½“ã® 8%)

### Cost ã®å•é¡Œ

- **ç¾çŠ¶**: $0.015/req (ç›®æ¨™: $0.010/req)
- **ã‚³ã‚¹ãƒˆå†…è¨³**:
  1. generate_response: $0.011 (73%)
  2. analyze_intent: $0.003 (20%)
  3. ãã®ä»–: $0.001 (7%)
- **ä¸»ãªè¦å› **: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ï¼ˆå¹³å‡ 800 tokensï¼‰

## æ”¹å–„ã®æ–¹å‘æ€§

### å„ªå…ˆåº¦ 1: analyze_intent ã®ç²¾åº¦å‘ä¸Š

- **å½±éŸ¿**: Accuracy ã«ç›´æ¥å½±éŸ¿ï¼ˆ-15%ã®ã‚®ãƒ£ãƒƒãƒ—ã® 60%ã‚’å ã‚ã‚‹ï¼‰
- **æ”¹å–„ç­–**: Few-shot examplesã€æ˜ç¢ºãªåˆ†é¡åŸºæº–ã€JSON å‡ºåŠ›å½¢å¼
- **æ¨å®šåŠ¹æœ**: +10-12% accuracy

### å„ªå…ˆåº¦ 2: generate_response ã®åŠ¹ç‡åŒ–

- **å½±éŸ¿**: Latency ã¨ Cost ã®ä¸¡æ–¹ã«å½±éŸ¿
- **æ”¹å–„ç­–**: ç°¡æ½”æ€§ã®æŒ‡ç¤ºã€max_tokens åˆ¶é™ã€temperature èª¿æ•´
- **æ¨å®šåŠ¹æœ**: -0.4s latency, -$0.004 cost
```

---

